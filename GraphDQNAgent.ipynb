{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNQklFp+GG4hXFaVLJ8vaT+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nhngmnh/RLAgentResearch/blob/master/GraphDQNAgent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BfcvNFXadjR",
        "outputId": "292dd83d-526e-4102-acdd-00ba9e7be55a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'initial_solution': [0, 5, 7, 6, 8, 2, 9, 4, 3, 1, 0], 'final_solution': [0, 4, 1, 3, 9, 7, 6, 2, 5, 8, 0], 'initial_distance': 516.0, 'final_distance': 367.0, 'execution_time': 20.139946937561035}\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from collections import deque\n",
        "\n",
        "# ---------------------------\n",
        "# Graph Q-Network (CPU-only)\n",
        "# ---------------------------\n",
        "class GraphQNetwork(nn.Module):\n",
        "    def __init__(self, feature_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(feature_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "\n",
        "    def forward(self, node_features, adjacency_matrix, current_index):\n",
        "        # node_features: [N, feature_dim], adjacency_matrix: [N,N]\n",
        "        h = F.relu(self.fc1(node_features))\n",
        "        h = F.relu(self.fc2(h))\n",
        "\n",
        "        degree = adjacency_matrix.sum(1, keepdim=True)\n",
        "        h_new = adjacency_matrix @ h / degree\n",
        "\n",
        "        h_cur = h_new[current_index]  # embedding current node\n",
        "        q_values = h_new @ h_cur      # Q-values for all nodes\n",
        "        return q_values\n",
        "\n",
        "# ---------------------------\n",
        "# Graph DQN Agent\n",
        "# ---------------------------\n",
        "class GraphDQNAgent:\n",
        "    def __init__(self, distance_matrix, feature_dim=3, hidden_dim=128,\n",
        "                 gamma=1.0, epsilon=1.0, decay=0.999, min_epsilon=0.01,\n",
        "                 episodes=500, batch_size=64, buffer_size=5000, lr=1e-3):\n",
        "        self.numCities = len(distance_matrix)\n",
        "        self.distanceMatrix = torch.tensor(distance_matrix, dtype=torch.float)\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.decay = decay\n",
        "        self.min_epsilon = min_epsilon\n",
        "        self.episodes = episodes\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.feature_dim = feature_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        # Networks\n",
        "        self.network = GraphQNetwork(feature_dim, hidden_dim)\n",
        "        self.target_network = GraphQNetwork(feature_dim, hidden_dim)\n",
        "        self.target_network.load_state_dict(self.network.state_dict())\n",
        "        self.target_network.eval()\n",
        "        self.optimizer = optim.Adam(self.network.parameters(), lr=lr)\n",
        "\n",
        "        # Replay buffer\n",
        "        self.replay_buffer = deque(maxlen=buffer_size)\n",
        "\n",
        "        # Best solutions\n",
        "        self.best_solution = None\n",
        "        self.best_distance = float('inf')\n",
        "        self.initial_solution = None\n",
        "        self.initial_distance = float('inf')\n",
        "\n",
        "    # -----------------------\n",
        "    # Node features and adjacency\n",
        "    # -----------------------\n",
        "    def get_node_features(self, visited_mask):\n",
        "        features = []\n",
        "        for i in range(self.numCities):\n",
        "            # [x, y, visited] (x=y=i chỉ là placeholder; nếu có coords thật, thay bằng coords)\n",
        "            features.append([i, i, 1 if visited_mask[i] else 0])\n",
        "        return torch.tensor(features, dtype=torch.float)\n",
        "\n",
        "    def get_adjacency(self):\n",
        "        return torch.ones((self.numCities, self.numCities))  # fully connected\n",
        "\n",
        "    # -----------------------\n",
        "    # Select action\n",
        "    # -----------------------\n",
        "    def select_action(self, node_features, adjacency_matrix, current_index, unvisited):\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.choice(list(unvisited))\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                q_values = self.network(node_features, adjacency_matrix, current_index)\n",
        "            q_np = q_values.numpy()\n",
        "            mask = np.array([0 if i in unvisited else 1 for i in range(self.numCities)])\n",
        "            q_np[mask == 1] = -np.inf\n",
        "            return int(np.argmax(q_np))\n",
        "\n",
        "    # -----------------------\n",
        "    # Store & train\n",
        "    # -----------------------\n",
        "    def store_transition(self, transition):\n",
        "        self.replay_buffer.append(transition)\n",
        "\n",
        "    def train_step(self):\n",
        "        if len(self.replay_buffer) < self.batch_size:\n",
        "            return\n",
        "        batch = random.sample(self.replay_buffer, self.batch_size)\n",
        "        loss = 0.0\n",
        "        for state_feat, adj, cur_idx, action, reward, next_feat, next_adj, next_idx, done, next_unvisited in batch:\n",
        "            q_pred = self.network(state_feat, adj, cur_idx)[action]\n",
        "            with torch.no_grad():\n",
        "                q_next = self.target_network(next_feat, next_adj, next_idx)\n",
        "                mask = np.array([0 if i in next_unvisited else 1 for i in range(self.numCities)])\n",
        "                q_next_np = q_next.numpy()\n",
        "                q_next_np[mask == 1] = -np.inf\n",
        "                max_q_next = 0 if done else np.max(q_next_np)\n",
        "                q_target = reward + self.gamma * max_q_next\n",
        "            loss += F.mse_loss(q_pred, torch.tensor(q_target))\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def decay_epsilon_func(self):\n",
        "        self.epsilon = max(self.min_epsilon, self.epsilon * self.decay)\n",
        "\n",
        "    # -----------------------\n",
        "    # Solve function\n",
        "    # -----------------------\n",
        "    def solve(self):\n",
        "        start_time = time.time()\n",
        "\n",
        "        for episode in range(self.episodes):\n",
        "            start = 0\n",
        "            state = start\n",
        "            unvisited = list(range(1, self.numCities))\n",
        "            visited_mask = [0]*self.numCities\n",
        "            visited_mask[start] = 1\n",
        "            path = [state]\n",
        "            total_reward = 0.0\n",
        "\n",
        "            while unvisited:\n",
        "                node_features = self.get_node_features(visited_mask)\n",
        "                adjacency_matrix = self.get_adjacency()\n",
        "\n",
        "                action = self.select_action(node_features, adjacency_matrix, state, unvisited)\n",
        "                reward = -self.distanceMatrix[state][action].item()\n",
        "                next_state = action\n",
        "                visited_mask[next_state] = 1\n",
        "                next_unvisited = [i for i in unvisited if i != action]\n",
        "\n",
        "                # store\n",
        "                self.store_transition((node_features, adjacency_matrix, state, action, reward,\n",
        "                                       self.get_node_features(visited_mask), adjacency_matrix, next_state, False, next_unvisited))\n",
        "                self.train_step()\n",
        "\n",
        "                state = next_state\n",
        "                path.append(state)\n",
        "                unvisited.remove(action)\n",
        "                total_reward += reward\n",
        "\n",
        "            # cuối tour: về start\n",
        "            reward = -self.distanceMatrix[state][start].item()\n",
        "            total_reward += reward\n",
        "            self.store_transition((self.get_node_features(visited_mask), adjacency_matrix, state, start, reward,\n",
        "                                   self.get_node_features(visited_mask), adjacency_matrix, start, True, []))\n",
        "            self.train_step()\n",
        "\n",
        "            # update best\n",
        "            tour_distance = -total_reward\n",
        "            if episode == 0:\n",
        "                self.initial_solution = [city for city in path] + [start]\n",
        "                self.initial_distance = tour_distance\n",
        "            if tour_distance < self.best_distance:\n",
        "                self.best_solution = [city for city in path] + [start]\n",
        "                self.best_distance = tour_distance\n",
        "\n",
        "            # decay epsilon\n",
        "            self.decay_epsilon_func()\n",
        "\n",
        "            # update target network\n",
        "            if episode % 10 == 0:\n",
        "                self.target_network.load_state_dict(self.network.state_dict())\n",
        "\n",
        "        end_time = time.time()\n",
        "        execution_time = end_time - start_time\n",
        "\n",
        "        return {\n",
        "            'initial_solution': self.initial_solution,\n",
        "            'final_solution': self.best_solution,\n",
        "            'initial_distance': self.initial_distance,\n",
        "            'final_distance': self.best_distance,\n",
        "            'execution_time': execution_time\n",
        "        }\n",
        "\n",
        "# -----------------------\n",
        "# Example usage\n",
        "# -----------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # distance matrix giả lập 10 thành phố\n",
        "    N = 10\n",
        "    distance_matrix = np.random.randint(1, 100, size=(N, N))\n",
        "    np.fill_diagonal(distance_matrix, 0)\n",
        "\n",
        "    agent = GraphDQNAgent(distance_matrix, episodes=50)\n",
        "    result = agent.solve()\n",
        "    print(result)\n"
      ]
    }
  ]
}